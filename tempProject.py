import os
import subprocess
import faiss
from sentence_transformers import SentenceTransformer
from datetime import datetime
from langdetect import detect

def detect_language(text):
    try:
        return detect(text)  # returns 'hi' or 'en'
    except:
        return "en"  # default fallback


DATA_DIR = "college_data"
MEMORY_FILE = "memory.txt"
EMBED_MODEL_PATH = "./embedding_models/all-MiniLM-L6-v2"
MAX_TOKENS = 200
HISTORY_DEPTH = 1

def load_documents(path=DATA_DIR):
    docs = []
    for root, _, files in os.walk(path):
        for file in files:
            if file.endswith(".txt"):
                with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read()
                    chunks = text.split("\n\n")
                    docs.extend([chunk.strip() for chunk in chunks if chunk.strip()])
    return docs

def load_memory():
    if os.path.exists(MEMORY_FILE):
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]
    return []

def save_memory_line(line):
    with open(MEMORY_FILE, "a", encoding="utf-8") as f:
        f.write(line.strip() + "\n")

os.environ["TOKENIZERS_PARALLELISM"] = "false"
print("[INFO] Loading embedding model...")
embed_model = SentenceTransformer(EMBED_MODEL_PATH)

print("[INFO] Loading documents...")
documents = load_documents() + load_memory()
print(f"[INFO] Loaded {len(documents)} chunks.")

print("[INFO] Embedding and indexing...")
embeddings = embed_model.encode(documents, normalize_embeddings=True)
index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

def retrieve_context(query, k=3):
    query_vec = embed_model.encode([query], normalize_embeddings=True)
    D, I = index.search(query_vec, k)
    return "\n---\n".join([documents[i] for i in I[0]])

def ask_llama(prompt):
    try:
        result = subprocess.run(
            ["ollama", "run", "llama3.2"],
            input=prompt.encode("utf-8"),
            capture_output=True,
            timeout=30
        )
        if result.returncode != 0:
            print("[ERROR] Ollama CLI error:", result.stderr.decode("utf-8").strip())
            return "[ERROR] Ollama model failed."
        response = result.stdout.decode("utf-8").strip()
        if not response:
            return "[ERROR] Ollama returned empty response."
        return response
    except subprocess.TimeoutExpired:
        return "[ERROR] Ollama call timed out."
    except Exception as e:
        return f"[ERROR] Exception: {e}"

chat_history = []
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
os.makedirs("chats", exist_ok=True)
chat_file_path = f"chats/chat_{timestamp}.txt"

def log_chat_to_file(user, bot):
    with open(chat_file_path, "a", encoding="utf-8") as f:
        f.write(f"User: {user}\nBot: {bot}\n\n")

def format_history():
    return "\n".join([
        f"User: {turn['user']}\nBot: {turn['bot']}"
        for turn in chat_history[-HISTORY_DEPTH:]
    ])

def build_prompt(query, context, lang="en"):
    if lang == "hi":
        identity = "‡§Ü‡§™ AlphaMind ‡§π‡•à‡§Ç, GEHU Bhimtal Campus ‡§ï‡•á ‡§Ü‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§ï ‡§∏‡§π‡§æ‡§Ø‡§ï‡•§\n‡§Ü‡§™‡§ï‡§æ ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡•ç‡§Ø ‡§ï‡•á‡§µ‡§≤ GEHU Bhimtal ‡§∏‡•á ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§®‡§æ ‡§π‡•à‡•§"
        communication_guidelines = """üìå ‡§∏‡§Ç‡§µ‡§æ‡§¶ ‡§¶‡§ø‡§∂‡§æ‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂:
"tone": "friendly"
"tone": "talkative"
"tone": "Humorous"
‡§Ü‡§™ AlphaMind ‡§π‡•à‡§Ç
- ‡§â‡§§‡•ç‡§§‡§∞ ‡§∏‡§Ç‡§ï‡•ç‡§∑‡§ø‡§™‡•ç‡§§ ‡§î‡§∞ ‡§™‡•ç‡§∞‡§æ‡§ï‡•É‡§§‡§ø‡§ï ‡§∞‡§ñ‡•á‡§Ç ‚Äî 1‚Äì2 ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡§Ø‡§æ‡§Å ‡§ú‡§¨ ‡§§‡§ï ‡§ï‡§ø ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ ‡§Ö‡§ß‡§ø‡§ï ‡§® ‡§Æ‡§æ‡§Ç‡§ó‡•á‡•§
- ‡§Æ‡§æ‡§®‡§µ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§¨‡§æ‡§§‡§ö‡•Ä‡§§ ‡§ï‡§∞‡•á‡§Ç, ‡§ï‡•ã‡§à ‡§∞‡•ã‡§¨‡•ã‡§ü‡§ø‡§ï ‡§≤‡§æ‡§á‡§® ‡§Ø‡§æ ‡§ú‡§¨‡§∞‡§¶‡§∏‡•ç‡§§‡•Ä ‡§Ö‡§≠‡§ø‡§µ‡§æ‡§¶‡§® ‡§® ‡§¶‡•á‡§Ç‡•§
- ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ï‡§≠‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§æ‡§® ‡§® ‡§≤‡§ó‡§æ‡§è‡§Ç ‚Äî ‡§ï‡•á‡§µ‡§≤ ‡§ú‡•ç‡§û‡§æ‡§§ ‡§§‡§•‡•ç‡§Ø‡•ã‡§Ç ‡§Ø‡§æ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠ ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•á‡§Ç‡•§
- ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§® ‡§¨‡§®‡§æ‡§è‡§Å‡•§"""
    else:
        identity = "You are AlphaMind, the official assistant for GEHU Bhimtal Campus.\nYour job is to respond accurately to queries related to the campus."
        communication_guidelines = """üìå Communication Guidelines:
"tone": "friendly"
"tone": "talkative"
"tone": "Humorous"
You are AlphaMind
- Keep replies short and natural ‚Äî 1‚Äì2 sentences unless the user asks for more.
- Respond conversationally like a human would. No robotic lines, no forced greetings.
- Never assume anything about the user ‚Äî only respond based on known facts or previous context.
- Do not make up information."""

    return f"""{identity}

{communication_guidelines}

Conversation History:
{format_history()}

Context:
{context}

User: {query}
Answer:"""


print("\nü§ñ CollegeBot is ready. Type 'exit' to quit.")
while True:
    query = input("\nüßë You: ")
    if query.lower() == "exit":
        break
    lang = detect_language(query)

    if query.lower().startswith(("remember that", "learn that")):
        fact = query.partition("that")[2].strip()
        if fact:
            save_memory_line(fact)
            print(f"\nAlphaMind: Got it! I‚Äôll remember: {fact}")
        else:
            print("\nPlease provide a fact after 'remember that'")
        continue

    context = retrieve_context(query)
    prompt = build_prompt(query, context, lang)
    answer = ask_llama(prompt)

    print(f"\nü§ñ CollegeBot: {answer}")
    chat_history.append({"user": query, "bot": answer})
    log_chat_to_file(query, answer)
